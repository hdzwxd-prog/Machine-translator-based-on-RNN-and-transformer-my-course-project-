# Transformer模型配置文件（RMSNorm版本）
# 用于架构消融实验：对比LayerNorm vs RMSNorm
data:
  data_dir: "/home/test/fxdproject/NLP_project/AP0004_MidtermFinal_translation_dataset_zh_en"
  train_file: "train_100k.jsonl"
  valid_file: "valid.jsonl"
  test_file: "test.jsonl"
  source_lang: "zh"
  target_lang: "en"
  max_length: 100
  min_freq: 2
  max_vocab_size: 50000

model:
  type: "transformer"
  pos_encoding_type: "absolute"
  norm_type: "rmsnorm"  # 使用RMSNorm
  use_relative_pos: false
  
  encoder:
    d_model: 512
    num_heads: 8
    num_layers: 6
    d_ff: 2048
    dropout: 0.1
    max_len: 5000
  
  decoder:
    d_model: 512
    num_heads: 8
    num_layers: 6
    d_ff: 2048
    dropout: 0.1
    max_len: 5000

training:
  batch_size: 32
  num_epochs: 30
  learning_rate: 0.0001
  lr_scheduler: "cosine"
  lr_step_size: 3
  lr_gamma: 0.5
  grad_clip: 1.0
  teacher_forcing_ratio: 1.0
  early_stopping_patience: 5
  save_dir: "checkpoints_transformer_rmsnorm"
  log_interval: 20
  eval_interval: 20
  plot_interval: 20
  use_ddp: false
  use_amp: true
  num_workers: 4
  pin_memory: true
  resume_from_checkpoint: false
  resume_ask_user: false

decoding:
  strategy: "beam_search"
  beam_size: 5
  max_length: 100
  length_penalty: 0.6

device: "cuda"
world_size: 1
dist_backend: "nccl"
seed: 42

